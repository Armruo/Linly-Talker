'''
pip install openai
'''
import os
import json
from openai import OpenAI

class Llama3():
    # å¯èƒ½çš„APIé…ç½®
    API_CONFIGS = [
        # Llama 3.3
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3.3-70b"
        },
        # Llama 3.2
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3.2-3b"
        },
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3.2-1b"
        },
        # Llama 3.1
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3.1-70b"
        },
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3.1-8b"
        },
        # Llama 3
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3-70b"
        },
        {
            "base_url": "https://api.llama-api.com",
            "model_name": "llama3-8b"
        },
        # å¤‡ç”¨ V1 APIç«¯ç‚¹
        {
            "base_url": "https://api.llama-api.com/v1",
            "model_name": "llama3.2-3b"
        },
        {
            "base_url": "https://api.llama-api.com/v1",
            "model_name": "llama3-70b"
        }
    ]
    
    def __init__(self, model_path = None, api_key = None, proxy_url = "", prefix_prompt = '''è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'''):
        self.history = []
        if proxy_url:
            os.environ['https_proxy'] = proxy_url if proxy_url else None
            os.environ['http_proxy'] = proxy_url if proxy_url else None
        
        if not api_key:
            print("è­¦å‘Šï¼šæœªæä¾›APIå¯†é’¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æµ‹è¯•å¯†é’¥")
            api_key = 'LA-bcde398ac3ef443c961f2f0af94be707d8506d0995fa47d09d3fa907fd53298c'
        elif not api_key.startswith('LA-'):
            raise ValueError("æ— æ•ˆçš„APIå¯†é’¥æ ¼å¼ã€‚Llama3 APIå¯†é’¥åº”è¯¥ä»¥'LA-'å¼€å¤´")
        
        # å°è¯•æ‰€æœ‰å¯èƒ½çš„APIé…ç½®
        last_error = None
        successful_models = []
        
        print("\nğŸ”„ å¼€å§‹æµ‹è¯•å¯ç”¨çš„Llamaæ¨¡å‹...")
        
        for config in self.API_CONFIGS:
            try:
                print(f"\nå°è¯•APIé…ç½®: {config['base_url']} with model {config['model_name']}")
                self.client = OpenAI(
                    api_key=api_key,
                    base_url=config['base_url']
                )
                # æµ‹è¯•APIè¿æ¥
                response = self.client.chat.completions.create(
                    model=config['model_name'],
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=1
                )
                print(f"âœ… æˆåŠŸè¿æ¥åˆ°Llama APIï¼Œä½¿ç”¨æ¨¡å‹ï¼š{config['model_name']}")
                successful_models.append(config['model_name'])
                self.model_path = config['model_name']
                break
            except Exception as e:
                error_msg = str(e)
                if "Invalid model name" in error_msg:
                    print(f"âŒ æ¨¡å‹åç§°æ— æ•ˆ: {config['model_name']}")
                elif "404" in error_msg:
                    print(f"âŒ APIç«¯ç‚¹ä¸å­˜åœ¨: {config['base_url']}")
                elif "401" in error_msg:
                    print(f"âŒ APIå¯†é’¥æ— æ•ˆæˆ–æœªæˆæƒ")
                    break  # APIå¯†é’¥é—®é¢˜ï¼Œåœæ­¢å°è¯•å…¶ä»–é…ç½®
                else:
                    print(f"âŒ é…ç½®å¤±è´¥: {error_msg}")
                last_error = e
                continue
        
        if last_error:
            print(f"\nâ— æ‰€æœ‰APIé…ç½®éƒ½å¤±è´¥äº†ã€‚è¯·ç¡®ä¿ï¼š")
            print("1. APIå¯†é’¥æ­£ç¡®ä¸”æœ‰æ•ˆ")
            print("2. ç½‘ç»œè¿æ¥æ­£å¸¸")
            print("3. ä»£ç†è®¾ç½®æ­£ç¡®ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰")
            if successful_models:
                print(f"\nâœ… æˆåŠŸæµ‹è¯•è¿‡çš„æ¨¡å‹: {', '.join(successful_models)}")
            print(f"\næœ€åçš„é”™è¯¯: {last_error}")
            raise last_error
            
        self.prefix_prompt = prefix_prompt

    def generate(self, message, system_prompt="ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚"):
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
            self.history += [{
                    "role": "user", 
                    "content": self.prefix_prompt + message if self.prefix_prompt else message
                }]
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = ([{"role": "system", "content": system_prompt}] if system_prompt else []) + self.history
            print(f"\nLlama3 APIè¯·æ±‚:\næ¨¡å‹: {self.model_path}\næ¶ˆæ¯: {json.dumps(messages, ensure_ascii=False, indent=2)}")
            
            # å‘é€APIè¯·æ±‚
            response = self.client.chat.completions.create(
                model=self.model_path,
                messages=messages,
                temperature=0.7,
                max_tokens=200
            )
            print(f"\nLlama3 APIåŸå§‹å“åº”:\n{response}")
            
            # å¤„ç†å“åº”
            if hasattr(response, 'choices') and len(response.choices) > 0:
                content = response.choices[0].message.content
            elif isinstance(response, dict) and 'choices' in response:
                content = response['choices'][0]['message']['content']
            elif isinstance(response, list) and len(response) > 0:
                # å¤„ç†åˆ—è¡¨ç±»å‹çš„å“åº”
                if isinstance(response[0], dict) and 'message' in response[0]:
                    content = response[0]['message']['content']
                elif isinstance(response[0], dict) and 'content' in response[0]:
                    content = response[0]['content']
                else:
                    raise ValueError(f"æ— æ³•ä»å“åº”ä¸­æå–å†…å®¹: {response}")
            else:
                raise ValueError(f"æ„å¤–çš„APIå“åº”æ ¼å¼: {response}")
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²è®°å½•
            self.history += [{
                "role": "assistant",
                "content": content
            }]
            
            return content
            
        except Exception as e:
            print(f"\nLlama3 APIé”™è¯¯: {str(e)}")
            return f"å¯¹ä¸èµ·ï¼Œè°ƒç”¨Llama3 APIæ—¶å‡ºé”™: {str(e)}\nè¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥ï¼Œç„¶åé‡è¯•ã€‚"

    def chat(self, system_prompt = "ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚", message = "", history=[]):
        response = self.generate(message, system_prompt)
        history.append((message, response))
        return response, history
        
    def clear_history(self):
        self.history = []
        
""" if __name__ == '__main__':
    API_KEY = 'LA-bcde398ac3ef443c961f2f0af94be707d8506d0995fa47d09d3fa907fd53298c'
    # è‹¥ä½¿ç”¨ChatGPTï¼Œè¦ä¿è¯è‡ªå·±çš„APIKEYå¯ç”¨ï¼Œå¹¶ä¸”æœåŠ¡å™¨å¯è®¿é—®OPENAI
    llm = Llama3(api_key=API_KEY, proxy_url=None)
    answer = llm.generate("å¦‚ä½•åº”å¯¹å‹åŠ›ï¼Ÿ")
    print(answer) """


def test():
    API_KEY = 'LA-bcde398ac3ef443c961f2f0af94be707d8506d0995fa47d09d3fa907fd53298c'
    # è‹¥ä½¿ç”¨ChatGPTï¼Œè¦ä¿è¯è‡ªå·±çš„APIKEYå¯ç”¨ï¼Œå¹¶ä¸”æœåŠ¡å™¨å¯è®¿é—®OPENAI
    llm = Llama3(api_key=API_KEY, proxy_url=None)
    answer, history = llm.chat("", "ä½ ä¼šè¯´æ³°è¯­ä¹ˆ")
    print(answer, history)
    from time import sleep
    sleep(5)
    answer, history = llm.chat("", "ç”¨æ³°è¯­ä»‹ç»ä¸€ä¸‹åŒºå—é“¾")
    print(answer, history)


if __name__ == '__main__':
    test()